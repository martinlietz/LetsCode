{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidades do Scikit-Learn\n",
    "O Scikit-Learn é um dos pacotes mais conhecidos do mundo de Ciência de Dados por trazer diversas funcionalidades práticas para o dia a dia de qualquer cientista. Além dos modelos já implementados, como Árvores de Decisão, KNN, e Regressão Logística, trás também praticidades para pré-processamento de dados, que, se bem utilizados, mitigarão riscos de data leakage (vazamento de dados), além de trazer ganhos produtivos.\n",
    "\n",
    "    Vazamento de dados no contexto de Machine Learning se refere ao erro do Cientista de Dados de acidentalmente compartilhar informações entre os conjuntos de teste e treino, causando uma avaliação possivelmente incorreta no desempenho do modelo preditivo.\n",
    "\n",
    "2. Considerações Gerais\n",
    "Muitos transformadores precisam de alguns parâmetros para definir como serão suas transformações. Porém, tais parâmetros devem ser ajustados observando apenas o conjunto de treino, exatamente para evitar o data leakage. Dessa forma, é importante separar bem qual é a base de treino e qual é a base teste. Com isso definido, instanciamos um objeto da classe do transformador e ajustamos os parâmetros usando o médoto fit(). Alguns transformadores exigem a variável resposta, outros não. Com os parâmetros encontrados, aplicamos o método transform() para efetivamente fazer a transformação. Dessa forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "transformador = Transformador()\n",
    "transformador.fit(x_treino, y_treino) \n",
    "x_treino_transformado = transformador.transform(x_treino)\n",
    "x_teste_transformado = transformador.transform(x_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe ainda o método fit_transform(), que basicamente chama o fit() e logo depois o trasnform(), retornando a tabela processada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformador = Transformador()\n",
    "x_treino_transformado = transformador.fit_transform(x_treino, y_treino) \n",
    "x_teste_transformado = transformador.transform(x_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem diversos transformadores, cada um com suas particularidades e usos específicos e podem ser acessados na seção de Transformação do User Guide do sklearn. A parte de pipelines será abordada em outro momento desse módulo, enquanto que a extração de atributos será tema de outro módulo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Binarizer\n",
    "\n",
    "Muitas vezes, uma variável contínua precisa ser categorizada por não ter uma boa distribuição dos seus valores, ou ainda por um conhecimento especialista do cenário no qual ela está inserida. Por exemplo, se estamos tratando de seguros de automóveis, podemos categorizar as idades dos contratantes em <= 25 e > 25. Com o sklearn temos a classe Binarizer que dado um determinado limiar (parâmetro threshold, que por default é igual a 0), transforma uma variável contínua em binária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold = 25)\n",
    "x_treino.loc[:, ['idade']] = binarizer.fit_transform(x_treino.loc[:, ['idade']]) \n",
    "x_teste.loc[:, ['idade']] = binarizer.transform(x_teste.loc[:, ['idade']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. KBinsDiscretizer\n",
    "\n",
    "Em algumas situações, ao transformar uma variável contínua em binária perdemos muita informação a ponto de prejudicar muito a modelagem. Podemos então utilizar o KBinsDiscretizer para realizar uma discretização não tão severa quando o Binarizer. Como parâmetros, podemos usar\n",
    "\n",
    "    n_bins: quantidade de valores discretos a ser criado;\n",
    "    encode: formato de saída, que pode ser estilo one hot ou ordinal em uma coluna só;\n",
    "    strategy: estratégia a ser usada para definir os limites, podendo ser uniforme, quantílico, ou até mesmo usar um K-Means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kbins = KBinsDiscretizer(n_bins = 4, enconde = 'ordinal', strategy = 'quantile')\n",
    "x_treino.loc[:, ['idade']] = kbins.fit_transform(x_treino.loc[:, ['idade']]) \n",
    "x_teste.loc[:, ['idade']] = kbins.transform(x_teste.loc[:, ['idade']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. MinMaxScaler\n",
    "\n",
    "Muitos modelos são sensíveis a escalas diferentes muito altas das variáveis explicativas, como o KNN. Nesses casos, podemos fazer um reescalamento dos dados, deixando todas as features no intervalo de, por exemplo, 0 e 1, utilizando o MinMaxScaler. Casos outros intervalos sejam desejados, alteramos o parâmetro feature_range, passando uma tupla com o mínimo e o máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler(feature_range = (0, 1))\n",
    "x_treino.loc[:, ['idade']] = minmax.fit_transform(x_treino.loc[:, ['idade']]) \n",
    "x_teste.loc[:, ['idade']] = minmax.transform(x_teste.loc[:, ['idade']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as mesmas motivações de reescalamento do MinMaxScaler, porém padronizando \n",
    "a distribuição da variável para $x=0$ e $\\sigma=1$ após a transformação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada elemento transformado é dado pela equação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ x'= \\frac{x-\\bar{x}} {\\sigma} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdscaler = StandardScaler()\n",
    "x_treino.loc[:, ['idade']] = stdscaler.fit_transform(x_treino.loc[:, ['idade']]) \n",
    "x_teste.loc[:, ['idade']] = stdscaler.transform(x_teste.loc[:, ['idade']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. OneHotEncoder\n",
    "\n",
    "Em algum momentos, pode ser necessário levar as variáveis categóricas de um domínio ordinal, usando apenas uma coluna, para uma representação clara de não-ordenação, no estilo de dummies, onde cada coluna é uma flag representando se uma determinada instância é de uma determinada categoria. É similar ao pd.get_dummies(), porém com suporte aos métodos fit() e transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "x_treino_ohe = ohe.fit_transform(x_treino.loc[:, ['cor']]) \n",
    "x_teste_ohe = ohe.transform(x_teste.loc[:, ['cor']]) \n",
    "\n",
    "x_treino = pd.concat([x_treino, pd.DataFrame(x_treino_ohe)], axis = 1)\n",
    "x_teste = pd.concat([x_teste, pd.DataFrame(x_teste_ohe)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. OrdinalEncoder\n",
    "\n",
    "Transforma uma variável categórica num formato numérico ordinal, detro do intervalo [0,ni−1]\n",
    ", onde ni é a quantidade de categorias para cada atributo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "x_treino.loc[:, ['idade']] = oe.fit_transform(x_treino.loc[:, ['idade']]) \n",
    "x_teste.loc[:, ['idade']] = oe.transform(x_teste.loc[:, ['idade']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. SimpleImputer\n",
    "\n",
    "Muitos modelos não trabalham bem com valores missing. Nesses casos, é importante utilizar de alguma estratégia de imputação de valores. O SimpleImputer possui diversas facilidades nesse sentido graças à sua versatilidade trazidas pelos seus parâmetros:\n",
    "- ```missing_values```: o indicador de missing, que por padrão é np.nan, mas pode ser ajustado para -999 ou '', por exemplo;\n",
    "- ```strategy```: qual a estratégia de imputação a ser utilizada, podendo incluir a média ('mean'), mediana ('median') e moda (most_frequent), todos desconsiderando missing em seus cálculos, ou ainda um valor constante (constant), a ser definido pelo parâmetro fill_value;\n",
    "- ```fill_value```: valor utilizado caso seja strategy esteja definido como 'constant'.\n",
    "- ```add_indicator```: muitas vezes, ser missing é uma ótima feature, e caso esse parâmetro seja setado como True, o transformador criará uma nova coluna para cada feature que foi realizada a imputação, indicando onde originalmente era missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SimpleImputer\n",
    "\n",
    "si = SimpleImputer(missing_values = -999, strategy = 'median', add_indicator = True)\n",
    "x_treino_si = si.fit_transform(x_treino.loc[:, ['idade']]) \n",
    "x_teste_si = si.transform(x_teste.loc[:, ['idade']]) \n",
    "\n",
    "x_treino = pd.concat([x_treino, pd.DataFrame(x_treino_si)], axis = 1)\n",
    "x_teste = pd.concat([x_teste, pd.DataFrame(x_teste_si)], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "04635d289a519a1410467dd0afb0db42f9184808881ca68b2eb5a687a20a5a94"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><markdown translate=\"no\" mathjax=\"\" class=\"ng-tns-c364-40\"><h1 id=\"processamento-de-linguagem-natural-pln\"><strong>Processamento de Linguagem Natural (PLN)</strong></h1>\n",
    "<h2 id=\"conceitos-fundamentais\"><strong>Conceitos Fundamentais</strong></h2>\n",
    "<h3 id=\"índice\">Índice</h3>\n",
    "<ul>\n",
    "<li>1) Introdução</li>\n",
    "<li>2) Aplicações</li>\n",
    "<li>3) Pré processamento</li>\n",
    "<li>4) PLN em Python: SpaCy</li>\n",
    "<li>5) Partes do discurso (POS - Parts Of Speech)</li>\n",
    "<li>6) Exercícios</li>\n",
    "</ul>\n",
    "<h3 id=\"1-introdução\"><strong>1) Introdução</strong></h3>\n",
    "<p>Processamento de linguagem natural é uma subárea de Inteligência artificial que tem como objetivo extrair informação a partir de <strong>linguagem natural</strong>, isto é, a linguagem do dia-a-dia de seres humanos.</p>\n",
    "<p>Assim, PLN compreende a análise e processamento tanto de textos (mensagens, tweets, notícias, documentos, livros, etc.) quanto de áudios transcritos (músicas, filmes, conversas telefônicas, etc.).</p>\n",
    "<p>PLN pode ser definido como uma forma de interpretar quem fez o quê, quando, onde, como e porque.</p>\n",
    "<p>Algumas das estratégias de PLN incluem usar conceitos linguísticos como análise sintática, semântica, léxica e morfológica da língua em questão. </p>\n",
    "<p>O objetivo é o de criar resumos, extrair informação de textos, interpretar sentidos, analisar sentimentos e aprender conceitos a partir da linguagem natural.</p>\n",
    "<h3 id=\"2-aplicações\"><strong>2) Aplicações</strong></h3>\n",
    "<p>Há diversas aplicações  para as técmicas de PLN. Entre elas, temos:</p>\n",
    "<ul>\n",
    "<li><strong>Tradução automática</strong>, como a utilizada pelo Google Tradutor;</li>\n",
    "<li><strong>Resumo de textos</strong>, por exemplo, textos associados a um processo jurídico;</li>\n",
    "<li><strong>Extração contextual</strong> para fins de análise econômica de determinado período histórico, por exemplo;</li>\n",
    "<li><strong>Análise de sentimentos</strong> para identificar a satisfação de usuários para com determinado produto;</li>\n",
    "<li><strong>Detecção de Spam</strong> em filtros de e-mail;</li>\n",
    "<li><strong>Chatbots</strong> para a automatização de atendimento...</li>\n",
    "</ul>\n",
    "<p>...e muitas outras!</p>\n",
    "<h3 id=\"3-pré-processamento\"><strong>3) Pré processamento</strong></h3>\n",
    "<p>Assim como em outras áreas de processamento de sinais, é necessário o pré processamento dos textos como parte das técnicas de PLN. As principais etapas de pré-processamento de textos são:</p>\n",
    "<ul>\n",
    "<li><p>1) <strong>Normalização/Tokenização:</strong> Divisão de um texto em suas componentes mínimas, como: palavras, frases, etc. Chamamos estas componentes de \"Tokens\";</p>\n",
    "</li>\n",
    "<li><p>2) <strong>Remoção de Stop Words:</strong> Palavras comuns que normalmente não contribuem muito para a interpretação do texto, como preposições e artigos, \"a\", \"de\", \"para\", etc.;</p>\n",
    "</li>\n",
    "<li><p>3) <strong>Remoção de numerais e pontuação</strong> como \"!\", \"?\", etc., caso estes não sejam relevantes para o contexto do problema;</p>\n",
    "</li>\n",
    "<li><p>4) <strong>Stemização e Lematização:</strong> Reduzir a palavra a seu radical linguístico.</p>\n",
    "</li>\n",
    "</ul>\n",
    "<p>A seguir, vamos aplicar as etapas descritas acima em Python!</p>\n",
    "<h3 id=\"4-pln-em-python-spacy\"><strong>4) PLN em Python: SpaCy</strong></h3>\n",
    "<p>SpaCy é um pacote (biblioteca) do Python bem popular que usaremos neste curso para apresentarmos as ferramentas e conceitos de PLN.</p>\n",
    "<p><strong>Instalação da biblioteca</strong></p>\n",
    "<p>Na linha de comando digite:</p>\n",
    "<p><code>pip install -U spacy</code></p>\n",
    "<p><code>python -m spacy download pt # para o modelo em português</code></p>\n",
    "<p><code>python -m spacy download pt_core_news_sm</code></p>\n",
    "<p>Os comandos acima irão instalar o pacote e baixar o modelo linguístico em português, que contém diversas ferramentas para PLN em português. Outros idiomas também podem ser selecionados, caso necessário.</p>\n",
    "<p><strong>Importando a biblioteca</strong></p>\n",
    "<p>Para importar a biblioteca e o modelo linguístico do Português, fazemos:</p>\n",
    "<pre class=\" language-python\"><code class=\" language-python\"><span class=\"token comment\"># importando o spacy </span>\n",
    "<span class=\"token keyword\">import</span> spacy\n",
    "<span class=\"token comment\"># carregando o modelo em pt</span>\n",
    "nlp <span class=\"token operator\">=</span> spacy<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">'pt_core_news_sm'</span><span class=\"token punctuation\">)</span>\n",
    "doc <span class=\"token operator\">=</span> nlp<span class=\"token punctuation\">(</span><span class=\"token string\">u'Esse texto é meramente ilustrativo, .'</span><span class=\"token punctuation\">)</span></code></pre>\n",
    "<p><strong>Criando um doc object</strong></p>\n",
    "<p>Um \"doc object\" nada mais é que o texto a ser processado com as técnicas de PLN.</p>\n",
    "<p>Este texto em geral é lido de alguma base de dados. Mas, para exemplificarmos algumas etapas de pré-processamento, vamos tomar um texto bem simples, de apenas uma frase, \"Esse texto é meramente ilustrativo.\":</p>\n",
    "<pre class=\" language-python\"><code class=\" language-python\"><span class=\"token comment\"># criando um doc object, lembrando de usar o unicode u'' para funcionar corretamente</span>\n",
    "doc <span class=\"token operator\">=</span> nlp<span class=\"token punctuation\">(</span><span class=\"token string\">u'Esse texto é meramente ilustrativo.'</span><span class=\"token punctuation\">)</span></code></pre>\n",
    "<p><strong>Tokenização</strong></p>\n",
    "<p>No nosso <code>doc</code> temos uma conjunto de <code>tokens</code> e agora vamos fazer a separação de cada token em uma lista:</p>\n",
    "<pre class=\" language-python\"><code class=\" language-python\"><span class=\"token comment\"># vamos extrair os tokens, utilizando compreensão de listas em Python:</span>\n",
    "tokens <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>token <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">]</span>\n",
    "<span class=\"token comment\"># [] [Esse, texto, é, meramente, ilustrativo.]</span></code></pre>\n",
    "<p>O resultado da tokenização é uma lista com cada token que compõem o <code>doc</code>!</p>\n",
    "<p>** Um pouco mais sobre docs e tokens**</p>\n",
    "<p>Um <code>Doc</code> é um objeto de sequência de objetos do tipo <code>Token</code> e possui diversas informações do texto que contém.\n",
    "Token pode ser uma palavra, frase, pontuação, etc</p>\n",
    "<pre class=\" language-python\"><code class=\" language-python\">doc <span class=\"token operator\">=</span> nlp<span class=\"token punctuation\">(</span><span class=\"token string\">u'Você encontrou o livro que eu lhe falei, Carla?'</span><span class=\"token punctuation\">)</span>\n",
    "<span class=\"token comment\"># vamos analisar a frase da maneira mais simples: dividindo-a com o método `split` de qualquer string</span>\n",
    "doc<span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n",
    "<span class=\"token comment\"># perceba como a vírgula e o ponto de interrogação ficaram juntos às palavras</span>\n",
    "<span class=\"token comment\"># [] ['Você', 'encontrou', 'o', 'livro', 'que', 'eu', 'lhe', 'falei,', 'Carla?']</span>\n",
    "\n",
    "<span class=\"token comment\"># utilizando compreensão de lista, a separação é garantida! </span>\n",
    "<span class=\"token punctuation\">[</span>token <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">]</span>\n",
    "<span class=\"token comment\"># [] [Você, encontrou, o, livro, que, eu, lhe, falei, ,, Carla, ?]</span>\n",
    "\n",
    "<span class=\"token comment\"># se quisermos toenizar o objeto com strings, utilize o \"token.orth_\"</span>\n",
    "<span class=\"token punctuation\">[</span>token<span class=\"token punctuation\">.</span>orth_ <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">]</span>\n",
    "<span class=\"token comment\"># [] ['Você',</span>\n",
    "<span class=\"token comment\"># 'encontrou',</span>\n",
    "<span class=\"token comment\"># 'o',</span>\n",
    "<span class=\"token comment\"># 'livro',</span>\n",
    "<span class=\"token comment\"># 'que',</span>\n",
    "<span class=\"token comment\"># 'eu',</span>\n",
    "<span class=\"token comment\"># 'lhe',</span>\n",
    "<span class=\"token comment\"># 'falei',</span>\n",
    "<span class=\"token comment\"># ',',</span>\n",
    "<span class=\"token comment\"># 'Carla',</span>\n",
    "<span class=\"token comment\"># '?']</span>\n",
    "\n",
    "<span class=\"token comment\"># O spacy entende que existe uma diferamça entre palavra e pontuação</span>\n",
    "<span class=\"token comment\"># também podemos fazer a sua filtragem, como exemplo:</span>\n",
    "<span class=\"token punctuation\">[</span>token <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> token<span class=\"token punctuation\">.</span>is_punct<span class=\"token punctuation\">]</span>\n",
    "<span class=\"token comment\"># [] [Você, encontrou, o, livro, que, eu, lhe, falei, Carla]</span>\n",
    "</code></pre>\n",
    "<p><strong>Similaridade</strong></p>\n",
    "<p>O spacy também permite avaliar similaridade entre palavras com o método <code>.similarity</code> de um token.</p>\n",
    "<p>Quanto maior o valor, maior a similaridade entre as palavras.</p>\n",
    "<p>Ex: vamos avaliar a similaridade entre 3 palabras: \"você\", \"livro\" e \"eu\"</p>\n",
    "<pre class=\" language-python\"><code class=\" language-python\"><span class=\"token comment\"># criando a lista con os tokens</span>\n",
    "tokens <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>token <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">]</span>\n",
    "\n",
    "<span class=\"token comment\"># similaridade entre \"você\" (indice 0 da lista) \"eu\" (indice 5 da lista)</span>\n",
    "<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokens<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>similarity<span class=\"token punctuation\">(</span>tokens<span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n",
    "\n",
    "<span class=\"token comment\"># similaridade entre \"você\" (indice 0 da lista) \"livro\" (indice 3 da lista)</span>\n",
    "<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>tokens<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>similarity<span class=\"token punctuation\">(</span>tokens<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n",
    "\n",
    "<span class=\"token comment\"># como se vê, \"você\" é muito mais similar semanticamente a \"eu\" (0.270) do que \"livro\" (-0.033)</span>\n",
    "<span class=\"token comment\"># [] 0.27020037</span>\n",
    "<span class=\"token comment\"># -0.033028863</span></code></pre>\n",
    "<h3 id=\"5-partes-do-discurso-pos---parts-of-speech\"><strong>5) Partes do discurso (POS - Parts Of Speech)</strong></h3>\n",
    "<p>O spacy oferece uma ferramenta de classificação gramatical, que permite classificar determinado token, dentro do contexto do texto, entre as possíveis classes gramaticais, como por exemplo:</p>\n",
    "<ul>\n",
    "<li>Substantivos (noun);</li>\n",
    "<li>Verbos (verb);</li>\n",
    "<li>Adjetivos (adjective);</li>\n",
    "<li>Pontuação (punctuation);</li>\n",
    "<li>etc.</li>\n",
    "</ul>\n",
    "<p>Para isso, usamos os seguintes métodos:</p>\n",
    "<ul>\n",
    "<li>.orth_: retorna o token em string;</li>\n",
    "<li>.pos_: retorna a classe gramatical do token;</li>\n",
    "<li>.tag_: retorna uma tag com explicações contextuais acerca da classificação;</li>\n",
    "<li>spacy.explain(token.tag_) se disponível, é a função que formula uma explicação da tag, que, como podemos ver, não é muito evidente. Se não disponível, a função retorna \"None\"<pre class=\" language-python\"><code class=\" language-python\"><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">.</span>orth_<span class=\"token punctuation\">,</span> token<span class=\"token punctuation\">.</span>pos_<span class=\"token punctuation\">,</span> token<span class=\"token punctuation\">.</span>tag_<span class=\"token punctuation\">,</span> spacy<span class=\"token punctuation\">.</span>explain<span class=\"token punctuation\">(</span>token<span class=\"token punctuation\">.</span>tag_<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">]</span>\n",
    "<span class=\"token comment\"># [] [('Você', 'PRON', 'PERS|M/F|3S|NOM|@SUBJ&gt;', None),</span>\n",
    "<span class=\"token comment\"># ('encontrou', 'VERB', '&lt;mv&gt;|V|PS|3S|IND|@FS-STA', None),</span>\n",
    "<span class=\"token comment\"># ('o', 'DET', '&lt;artd&gt;|ART|M|S|@&gt;N', None),</span>\n",
    "<span class=\"token comment\"># ('livro', 'NOUN', '&lt;np-def&gt;|N|M|S|@&lt;ACC', None),</span>\n",
    "<span class=\"token comment\"># ('que', 'PRON', '&lt;rel&gt;|INDP|M|S|@SUBJ&gt;', None),</span>\n",
    "<span class=\"token comment\"># ('eu', 'PRON', 'PERS|M/F|1S|NOM|@SUBJ&gt;', None),</span>\n",
    "<span class=\"token comment\"># ('lhe', 'PRON', 'PERS|M|3S|DAT|@DAT&gt;', None),</span>\n",
    "<span class=\"token comment\"># ('falei', 'NOUN', '&lt;first-cjt&gt;|&lt;np-idf&gt;|N|F|S|@&lt;SC', None),</span>\n",
    "<span class=\"token comment\"># (',', 'PUNCT', 'PU|@PU', None),</span>\n",
    "<span class=\"token comment\"># ('Carla', 'PROPN', 'PROP|F|S|@APP', None),</span>\n",
    "<span class=\"token comment\"># ('?', 'PUNCT', 'PU|@PU', None)]</span></code></pre>\n",
    "</li>\n",
    "</ul>\n",
    "<h3 id=\"stemização-e-lematização\"><strong>Stemização e Lematização</strong></h3>\n",
    "<p>Imagine que você tem um texto enorme e diversos tempos verbais diferentes para um mesmo verbo.</p>\n",
    "<p>Dependendo do contexto, os diferentes tempos verbais podem não ser interessantes, de modo que importa mais a raiz (radical) do verbo.</p>\n",
    "<p>Por exemplo, para as fornmas verbais \"encontrei\", \"encontraram\", \"encontrarão\", \"encotrariam\", pode ser interessante extrairmos apenas o radical \"encontrar\".</p>\n",
    "<p>Esse é o processo da lematização, que é aplicado com o método \".lemma_\":</p>\n",
    "<pre class=\" language-python\"><code class=\" language-python\"><span class=\"token comment\"># retorna a versão lematizada de todos os verbos na lista de tokens</span>\n",
    "<span class=\"token comment\"># no caso, o único verbo é \"encontrou\", que é lematizado para o infinitivo \"encontrar\"</span>\n",
    "<span class=\"token punctuation\">[</span>token<span class=\"token punctuation\">.</span>lemma_ <span class=\"token keyword\">for</span> token <span class=\"token keyword\">in</span> doc <span class=\"token keyword\">if</span> token<span class=\"token punctuation\">.</span>pos_ <span class=\"token operator\">==</span> <span class=\"token string\">'VERB'</span><span class=\"token punctuation\">]</span>\n",
    "<span class=\"token comment\"># [] ['encontrar']</span></code></pre>\n",
    "<h3 id=\"entidades\"><strong>Entidades</strong></h3>\n",
    "<p>Outra ferramenta extremamente útil do spacy é a de identificação automática de entidades em frases.</p>\n",
    "<p>Entidades são classes particulares dadas à palavras, como nome de pessoas, nome de locais, objetos, etc.</p>\n",
    "<p>Podemos determinar a classe das entidades presentes em um texto utilizando o método \".label_\" de uma lista de entidades construídas pelo método \".ents\", aplicado ao <code>doc</code>. Por exemplo:</p>\n",
    "<pre class=\" language-python\"><code class=\" language-python\"><span class=\"token comment\"># definimos o doc object</span>\n",
    "doc <span class=\"token operator\">=</span> nlp<span class=\"token punctuation\">(</span><span class=\"token string\">u'Machado de Assis um dos melhores escritores do Brasil, foi o primeiro presidente da Academia Brasileira de Letras'</span><span class=\"token punctuation\">)</span>\n",
    "\n",
    "<span class=\"token comment\"># criamos uma lista de tuplas com a entidade e sua label</span>\n",
    "<span class=\"token comment\"># as entidades são determinadas através do método .ents aplicada ao doc</span>\n",
    "<span class=\"token comment\"># a classificação da entidade é acessada pelo método .label_</span>\n",
    "entidades <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>entidade<span class=\"token punctuation\">,</span> entidade<span class=\"token punctuation\">.</span>label_<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> entidade <span class=\"token keyword\">in</span> doc<span class=\"token punctuation\">.</span>ents<span class=\"token punctuation\">]</span>\n",
    "\n",
    "<span class=\"token comment\"># \"PER\" é a label da entidade de pessoa (person);</span>\n",
    "<span class=\"token comment\"># \"LOC\" é a label da entidade de lugar (location);</span>\n",
    "<span class=\"token comment\"># \"ORG\" é a label da entidade e organização (organization).</span>\n",
    "<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>entidades<span class=\"token punctuation\">)</span>\n",
    "<span class=\"token comment\"># [] [(Machado de Assis, 'PER'), (Brasil, 'LOC'), (Academia Brasileira de Letras, 'ORG')]</span></code></pre>\n",
    "<p>Podemos ver que o Spacy identificou:</p>\n",
    "<ul>\n",
    "<li>Machado de Assis como uma pessoa (PER de person, em inglês), </li>\n",
    "<li>Brasil como um local (LOC) e </li>\n",
    "<li>Academia Brasileira de Letras como uma organização (ORG).</li>\n",
    "</ul>\n",
    "<p>Vale ressaltar que o modelo linguístico em inglês é bem mais completo, contendo muito mais entidades e ferramentas de PLN.</p>\n",
    "<h3 id=\"6-exercícios\"><strong>6) Exercícios</strong></h3>\n",
    "<p>1) Indique os verbos de uma frase, usando o spacy;</p>\n",
    "<p>2) Indicar o infinitivo dos verbos com o processo de lematização;</p>\n",
    "<p>3) Indicar as classes gramaticais de todas as palavras.</p>\n",
    "</markdown></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "04635d289a519a1410467dd0afb0db42f9184808881ca68b2eb5a687a20a5a94"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
